{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import xml.etree.ElementTree as etree\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Tensorflow libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # voc dataset classes \n",
    "# voc_labels = {\n",
    "#     \"background\": 0,\n",
    "#     \"aeroplane\": 1,\n",
    "#     \"bicycle\": 2,\n",
    "#     \"bird\": 3,\n",
    "#     \"boat\": 4,\n",
    "#     \"bottle\": 5,\n",
    "#     \"bus\": 6,\n",
    "#     \"car\": 7,\n",
    "#     \"cat\": 8,\n",
    "#     \"chair\": 9,\n",
    "#     \"cow\": 10,\n",
    "#     \"dining_table\": 11,\n",
    "#     \"dog\": 12,\n",
    "#     \"horse\": 13,\n",
    "#     \"motorbike\": 14,\n",
    "#     \"person\": 15,\n",
    "#     \"potted_plant\": 16,\n",
    "#     \"sheep\": 17,\n",
    "#     \"sofa\":18,\n",
    "#     \"train\":19,\n",
    "#     \"tvmonitor\": 20\n",
    "# }\n",
    "\n",
    "# filter the classes for training\n",
    "voc_labels = {\n",
    "    \"background\": 0,\n",
    "    \"person\": 1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    data_dir = \"/home/ash/Documents/Personal/Projects/odts/benchmarks/VOC2012\"\n",
    "    # data_dir = \"/home/ash/Documents/Projects/VOC2012\"\n",
    "    num_classes = len(voc_labels)\n",
    "    dropout_rate = 0.35\n",
    "    learning_rate = 0.0001\n",
    "    test_size = 0.1\n",
    "    drop_rate = 0.5\n",
    "    image_shape = (64,64)\n",
    "    batch_size = 32\n",
    "    epochs = 100\n",
    "    input_shape = (image_shape[0], image_shape[1], 3)   \n",
    "    optimizer=\"Adam\"\n",
    "    loss=\"categorical_crossentropy\"\n",
    "    metrics=[\"accuracy\"]\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFilepaths(DATA_DIR):\n",
    "    print(\"Getting paths for images and corrosponding lables\")\n",
    "\n",
    "    ANNOTATION_DIR = os.path.join(DATA_DIR, \"Annotations\")\n",
    "    IMAGES_DIR = os.path.join(DATA_DIR, \"JPEGImages\")\n",
    "\n",
    "    annotation_filenames = sorted(os.listdir(ANNOTATION_DIR))\n",
    "    image_paths = []\n",
    "    annotation_paths = []\n",
    "    for num, filename in enumerate(annotation_filenames):\n",
    "        annotation_path = os.path.join(ANNOTATION_DIR, filename)\n",
    "        image_filename = filename.split(\".\")[0]\n",
    "        image_path = os.path.join(IMAGES_DIR, image_filename + \".jpg\")\n",
    "        image_paths.append(image_path)\n",
    "        annotation_paths.append(annotation_path)\n",
    "    assert(len(image_paths) == len(annotation_filenames))\n",
    "    return image_paths, annotation_paths\n",
    "image_paths, annotation_paths = getFilepaths(args.data_dir)\n",
    "print(\"Images = \", len(image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parse the xml annotation file and retrieve the path to each image, its size and annotations\n",
    "def extract_xml_annotation(filename):\n",
    "    \"\"\"Parse the xml file\n",
    "    :param filename: str\n",
    "    \"\"\"\n",
    "    z = etree.parse(filename)\n",
    "    objects = z.findall(\"./object\")\n",
    "    size = (int(float(z.find(\".//width\").text)), int(float(z.find(\".//height\").text)))\n",
    "    fname = z.find(\"./filename\").text\n",
    "    dicts = [\n",
    "        {\n",
    "            obj.find(\"name\").text: [\n",
    "                int(float(obj.find(\"bndbox/xmin\").text)),\n",
    "                int(float(obj.find(\"bndbox/ymin\").text)),\n",
    "                int(float(obj.find(\"bndbox/xmax\").text)),\n",
    "                int(float(obj.find(\"bndbox/ymax\").text)),\n",
    "            ]\n",
    "        }\n",
    "        for obj in objects\n",
    "    ]\n",
    "    return {\"size\": size, \"filename\": fname, \"objects\": dicts}\n",
    "\n",
    "\n",
    "def loadAnnotation(filename):\n",
    "    annotation = extract_xml_annotation(filename)\n",
    "    return annotation\n",
    "\n",
    "\n",
    "def loadImage(filename):\n",
    "    image = cv2.imread(filename)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "def patch(axis, bbox, display_txt, color):\n",
    "    coords = (bbox[0], bbox[1]), bbox[2] - bbox[0] + 1, bbox[3] - bbox[1] + 1\n",
    "    axis.add_patch(plt.Rectangle(*coords, fill=False, edgecolor=color, linewidth=2))\n",
    "    axis.text(\n",
    "        bbox[0],\n",
    "        bbox[1],\n",
    "        display_txt,\n",
    "        color=\"white\",\n",
    "        bbox={\"facecolor\": color, \"alpha\": 0.5},\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_annotations(img, prediction=None, ground_truth=None):\n",
    "    current_axis = plt.gca()\n",
    "    for object in ground_truth[\"objects\"]:\n",
    "        obj_class = list(object.keys())[0]\n",
    "        bbox = list(object.values())[0]\n",
    "        if ground_truth:\n",
    "            text = \"GT \" + obj_class\n",
    "            patch(current_axis, bbox, text, \"red\")\n",
    "        if prediction:\n",
    "            conf = f'{prediction[\"confidence\"]:0.2f} '\n",
    "            text = \"PRED \" + obj_class\n",
    "            patch(current_axis, bbox, text, \"blue\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "idx = 1\n",
    "image = loadImage(image_paths[idx])\n",
    "annotation = loadAnnotation(annotation_paths[idx])\n",
    "print(annotation)\n",
    "plot_annotations(image, ground_truth=annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawRectangle(image, rectangle):\n",
    "    x, y, w, h = rectangle\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "def showRectangles(image, rectangles, first_n):\n",
    "    for i, rect in enumerate(rectangles):\n",
    "        # draw reactangles\n",
    "        drawRectangle(image, rect)\n",
    "        if i == first_n:\n",
    "            break\n",
    "    # Show image\n",
    "    plt.imshow(image)\n",
    "\n",
    "\n",
    "def getROI(image, rect):\n",
    "    x1, y1, x2, y2 = rect\n",
    "    roi = image[y1:y2, x1:x2]\n",
    "    return roi\n",
    "\n",
    "\n",
    "def bb_intersection_over_union(boxA, boxB):\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    if interArea == boxAArea + boxBArea:\n",
    "        iou = 1\n",
    "    else:\n",
    "        iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    # return the intersection over union value\n",
    "    return iou\n",
    "\n",
    "\n",
    "def selectiveSearch(method, image):\n",
    "    # segmentation\n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "\n",
    "    # set the image on which we will run segmentation\n",
    "    ss.setBaseImage(image)\n",
    "    # Switch to fast but low recall selective search method\n",
    "    if method == \"fast\":\n",
    "        ss.switchToSelectiveSearchFast()\n",
    "    elif method == \"quality\":\n",
    "        ss.switchToSelectiveSearchQuality()\n",
    "    else:\n",
    "        print(__doc__)\n",
    "        sys.exit(1)\n",
    "    # process the selective search segmentatation on the image\n",
    "    rectangles = ss.process()\n",
    "    # rectangles = rectangles.unique()\n",
    "    return rectangles[:2000]\n",
    "\n",
    "\n",
    "\n",
    "cv2.setUseOptimized(True)\n",
    "cv2.setNumThreads(4)\n",
    "idx = 1  # random.randint(0, len(images))\n",
    "image = loadImage(image_paths[idx])\n",
    "# Set the selective search object class\n",
    "rectangles = selectiveSearch(\"fast\", image)\n",
    "# # Show rectangles\n",
    "showRectangles(image, rectangles, first_n=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def roiExtractor(image, annotation, image_shape, voc_labels):\n",
    "    X = []  #\n",
    "    y = []  #\n",
    "    img_classes = [] # all the objec classes in the image\n",
    "    img_boxes = [] # Corrosponding boxes to the objects\n",
    "\n",
    "    # Get objects and their respective bounding boxes\n",
    "    for object in annotation[\"objects\"]:\n",
    "        obj_class = list(object.keys())[0]\n",
    "        if obj_class in voc_labels:\n",
    "            obj_box = list(object.values())[0]\n",
    "            img_classes.append(obj_class)\n",
    "            img_boxes.append(obj_box)\n",
    "    \n",
    "    # If no objects found in training labels, exit\n",
    "    if not img_classes:\n",
    "        return X, y\n",
    "\n",
    "    max_background_images = 5 * len(img_classes) # get only 5 background images per object\n",
    "    num_background_images = 0 # counter for bckground images\n",
    "    \n",
    "    # Get selective search proposals for the image\n",
    "    ss_results = selectiveSearch(\"fast\", image)\n",
    "    background_images = []\n",
    "    for ss_result in ss_results:\n",
    "        x1, y1, w, h = ss_result\n",
    "        ss_box = [x1, y1, x1+w, y1+h]\n",
    "        \n",
    "        #Get iou of the ss_box for each object class (filtered) in the image\n",
    "        iou_list = []\n",
    "        for idx in range(len(img_classes)):\n",
    "            iou = bb_intersection_over_union(ss_box, obj_box)\n",
    "            roi_class = img_classes[idx]\n",
    "            iou_list.append(iou)\n",
    "\n",
    "        # Get max of iou and corrosponding label\n",
    "        iou_max = max(iou_list)\n",
    "        if iou_max > 0.7:\n",
    "            # Get the roi and resize it to the input shape of nn\n",
    "            roi = getROI(image, ss_box) # crop the image\n",
    "            roi = cv2.resize(roi, (image_shape[0], image_shape[1])) # resize the image\n",
    "            roi_class = img_classes[iou_list.index(iou_max)]\n",
    "            roi_label = voc_labels.get(roi_class)\n",
    "            X.append(roi)\n",
    "            y.append(roi_label)\n",
    "        elif iou_max < 0.2 and num_background_images < max_background_images:\n",
    "            # Get the roi and resize it to the input shape of nn\n",
    "            roi = getROI(image, ss_box) # crop the image\n",
    "            roi = cv2.resize(roi, (image_shape[0], image_shape[1])) # resize the image\n",
    "            background_images.append(roi)\n",
    "            num_background_images += 1\n",
    "    # create labels for background images\n",
    "    background_labels = [0] * len(background_images) # label for background\n",
    "    X.extend(background_images)\n",
    "    y.extend(background_labels)\n",
    "    assert(len(X)==len(y))\n",
    "    return X, y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "image = loadImage(image_paths[idx])\n",
    "annotation = loadAnnotation(annotation_paths[idx])\n",
    "\n",
    "X_train_roi, y_train_roi = roiExtractor(image, annotation, image_shape=args.image_shape, voc_labels = voc_labels)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=5, figsize=[8, 8])\n",
    "for _, ax in enumerate(axes.flat):\n",
    "    i = random.randint(0, (len(X_train_roi)-1))\n",
    "    ax.imshow(X_train_roi[i])\n",
    "    roi_class = list(voc_labels.keys())[list(voc_labels.values()).index(y_train_roi[i])]\n",
    "    ax.set_title(roi_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def createDataset(image_paths, annotation_paths, image_shape, voc_labels):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for idx in tqdm(range(len(image_paths))):\n",
    "        image = loadImage(image_paths[idx])\n",
    "        annotation = loadAnnotation(annotation_paths[idx])\n",
    "        X, y = roiExtractor(image, annotation, image_shape, voc_labels)\n",
    "        images.extend(X)\n",
    "        labels.extend(y)\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "images, labels = createDataset(image_paths, annotation_paths, args.image_shape, voc_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = np.array(images)\n",
    "print(labels[0:5])\n",
    "y = np.array(labels)\n",
    "y = tf.keras.utils.to_categorical(labels, len(voc_labels))\n",
    "# split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=args.test_size, random_state=42)\n",
    "\n",
    "print(\"Training images = \", len(X_train))\n",
    "print(\"Test images = \", len(X_test))\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data generator\n",
    "trdata = tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rotation_range=90)\n",
    "traindata = trdata.flow(x=X_train, y=y_train)\n",
    "# Tests data generator\n",
    "tsdata = tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=False, vertical_flip=False, rotation_range=0)\n",
    "testdata = tsdata.flow(x=X_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Keras\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "# dynamically grow the memory used on the GPU\n",
    "# to log device placement (on which device the operation ran)\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "# set this TensorFlow session as the default session for Keras\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the backbone model i.e. vgg16\n",
    "class Backbone:\n",
    "    \"\"\"Get the VGG network as a backbone.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        arch,\n",
    "        include_top,\n",
    "        weights,\n",
    "        input_shape,\n",
    "        trainable,\n",
    "    ):\n",
    "\n",
    "        self.arch = arch\n",
    "        self.include_top = include_top\n",
    "        self.weights = weights\n",
    "        self.input_tensor = None\n",
    "        self.input_shape = input_shape\n",
    "        self.pooling = None\n",
    "        self.classes = 1000\n",
    "        self.trainable = trainable\n",
    "\n",
    "    def vgg16(self):\n",
    "        return tf.keras.applications.VGG16(\n",
    "            include_top=self.include_top,\n",
    "            weights=self.weights,\n",
    "            input_tensor=self.input_tensor,\n",
    "            input_shape=self.input_shape,\n",
    "            pooling=self.pooling,\n",
    "            classes=self.classes,\n",
    "        )\n",
    "\n",
    "    def vgg19(self):\n",
    "        return tf.keras.applications.VGG19(\n",
    "            include_top=self.include_top,\n",
    "            weights=self.weights,\n",
    "            input_tensor=self.input_tensor,\n",
    "            input_shape=self.input_shape,\n",
    "            pooling=self.pooling,\n",
    "            classes=self.classes,\n",
    "        )\n",
    "\n",
    "    def resnet50(self):\n",
    "        return tf.keras.applications.ResNet50(\n",
    "            include_top=self.include_top,\n",
    "            weights=self.weights,\n",
    "            input_tensor=self.input_tensor,\n",
    "            input_shape=self.input_shape,\n",
    "            pooling=self.pooling,\n",
    "            classes=self.classes,\n",
    "        )\n",
    "\n",
    "    def resnet101(self):\n",
    "        return tf.keras.applications.ResNet101(\n",
    "            include_top=self.include_top,\n",
    "            weights=self.weights,\n",
    "            input_tensor=self.input_tensor,\n",
    "            input_shape=self.input_shape,\n",
    "            pooling=self.pooling,\n",
    "            classes=self.classes,\n",
    "        )\n",
    "\n",
    "    def densenet121(self):\n",
    "        return tf.keras.applications.DenseNet121(\n",
    "            include_top=self.include_top,\n",
    "            weights=self.weights,\n",
    "            input_tensor=self.input_tensor,\n",
    "            input_shape=self.input_shape,\n",
    "            pooling=self.pooling,\n",
    "            classes=self.classes,\n",
    "        )\n",
    "\n",
    "    def mobilenetV2(self):\n",
    "        return tf.keras.applications.MobileNetV2(\n",
    "            include_top=self.include_top,\n",
    "            weights=self.weights,\n",
    "            input_tensor=self.input_tensor,\n",
    "            input_shape=self.input_shape,\n",
    "            pooling=self.pooling,\n",
    "            classes=self.classes,\n",
    "        )\n",
    "\n",
    "    def backboneModel(self):\n",
    "        if self.arch == \"vgg16\":\n",
    "            model = self.vgg16()\n",
    "        elif self.arch == \"vgg19\":\n",
    "            model = self.vgg19()\n",
    "        elif self.arch == \"resnet50\":\n",
    "            model = self.resnet50()\n",
    "        elif self.arch == \"resnet101\":\n",
    "            model = self.resnet101()\n",
    "        elif self.arch == \"densenet121\":\n",
    "            model = self.densenet121()\n",
    "        elif self.arch == \"mobilenetV2\":\n",
    "            model = self.mobilenetV2()\n",
    "        else:\n",
    "            print(\"Invalid architecture\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        model.trainable = self.trainable\n",
    "        return model\n",
    "\n",
    "arch = \"densenet121\"\n",
    "include_top = True\n",
    "weights = \"imagenet\"\n",
    "input_shape = (224, 224, 3)\n",
    "trainable = False\n",
    "optimizer = \"Adam\"\n",
    "loss = \"categorical_crossentropy\"\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "\n",
    "# Get the backbone model eg. vgg16, densenet121 etc\n",
    "backbone = Backbone(\n",
    "    arch=\"vgg16\",\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=args.input_shape,\n",
    "    trainable=False,\n",
    ")\n",
    "backbone_model = backbone.backboneModel()\n",
    "backbone_model.compile(optimizer, loss, metrics)\n",
    "backbone_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model(backbone_model, output_classes, dropout_rate):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(backbone_model)\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(output_classes, activation=\"softmax\"))\n",
    "    return model\n",
    "\n",
    "## Create model\n",
    "\n",
    "# Get the sequential model with trainable head\n",
    "model = Model(backbone_model=backbone.backboneModel(), output_classes=args.num_classes, dropout_rate=args.dropout_rate)\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=args.optimizer,\n",
    "    loss=args.loss,\n",
    "    metrics=args.metrics,\n",
    ")\n",
    "\n",
    "# Publish the model summary\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checkpoints for saving the model\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"person_model.h5\",\n",
    "    verbose=1,\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode=\"auto\",\n",
    ")\n",
    "\n",
    "early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=100, verbose=1, mode='auto')\n",
    "\n",
    "# Train the model\n",
    "model.fit(traindata,\n",
    "        steps_per_epoch=len(X_train) // args.batch_size,\n",
    "        epochs=10,#args.epochs,\n",
    "        callbacks=[checkpoint,early],\n",
    "        validation_data=testdata,\n",
    "        verbose=1, shuffle=True)\n",
    "\n",
    "model.save(\"person_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
